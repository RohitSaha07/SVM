{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b2df2d",
   "metadata": {},
   "source": [
    "# Supervised Classification: Decision Trees, SVM, and Naive Bayes Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is Information Gain, and how is it used in Decision Trees?\n",
    "'''\n",
    "Information Gain is a metric used in Decision Trees to decide which feature to split on at each node.\n",
    "It measures the reduction in uncertainty about the target variable after splitting the data based on a feature.\n",
    "Information Gain is based on entropy, which measures randomness or impurity in a dataset.\n",
    "How Information Gain Is Used in Decision Trees -\n",
    "1. Start with the full dataset at the root node.\n",
    "2. Compute entropy of the target variable.\n",
    "3. For each feature: Split the data based on that feature, Compute the weighted entropy of the resulting subsets.\n",
    "   and Calculate Information Gain.\n",
    "4. Choose the feature with the highest Information Gain for the split.\n",
    "5. Repeat the process recursively for child nodes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37504379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the difference between Gini Impurity and Entropy?\n",
    "'''\n",
    "1. Gini Impurity measures the probability of incorrectly classifying a randomly chosen data point \n",
    "if it were labeled according to the class distribution in the node.\n",
    "Entropy measures the amount of uncertainty or randomness in the data.\n",
    "2. Computational speed of Gini Impurity is faster but slower for Entropy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is Pre-Pruning in Decision Trees?\n",
    "'''\n",
    "Pre-pruning is a technique used in Decision Trees to stop the growth of the tree early, \n",
    "before it perfectly fits the training data.\n",
    "Its main goal is to prevent overfitting and improve generalization on unseen data.\n",
    "Pre-Pruning is needed for -\n",
    "1. Fully grown trees memorize training data.\n",
    "2. Leads to high variance.\n",
    "3. Poor performance on test data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c64476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "Area : 1.0\n",
      "Rooms: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 4. Write a Python program to train a Decision Tree Classifier using Gini Impurity \n",
    "# as the criterion and print the feature importances.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Features: [Area, Rooms]\n",
    "X = np.array([\n",
    "    [1200, 2],\n",
    "    [1500, 3],\n",
    "    [1800, 3],\n",
    "    [2000, 4],\n",
    "    [2200, 4]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X, y)\n",
    "print(\"Feature Importances:\")\n",
    "print(\"Area :\", model.feature_importances_[0])\n",
    "print(\"Rooms:\", model.feature_importances_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What is a Support Vector Machine (SVM)?\n",
    "'''\n",
    "A Support Vector Machine is a supervised machine learning algorithm used for classification and regression \n",
    "that finds the optimal decision boundary which best separates data points of different classes.\n",
    "The goal of SVM is to maximize the margin.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What is the Kernel Trick in SVM?\n",
    "'''\n",
    "The Kernel Trick is a technique used in Support Vector Machines that allows the algorithm to separate \n",
    "non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space, \n",
    "without explicitly computing the transformation.\n",
    "The kernel trick helps SVM draw non-linear decision boundaries efficiently.\n",
    "Kernel Trick is needed -\n",
    " 1. Some datasets cannot be separated by a straight line in the original feature space.\n",
    " 2. By transforming data into a higher dimension, it becomes linearly separable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd45b61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Linear Kernel SVM: 0.9814814814814815\n",
      "Accuracy using RBF Kernel SVM: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, \n",
    "# then compare their accuracies.\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
    "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
    "print(\"Accuracy using Linear Kernel SVM:\", linear_accuracy)\n",
    "print(\"Accuracy using RBF Kernel SVM:\", rbf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What is the Naive Bayes classifier, and why is it called \"Naive\"?\n",
    "'''\n",
    "The Naïve Bayes classifier is a supervised probabilistic classification algorithm based on Bayes’ Theorem.\n",
    "It predicts the class of a data point by computing the posterior probability of each class \n",
    "given the input features and selecting the class with the highest probability.\n",
    "It is called naive because it makes a strong and simplifying assumption that\n",
    "All features are conditionally independent given the class label.\n",
    "How Naive Bayes Works -\n",
    "1. Calculate prior probabilities for each class\n",
    "2. Calculate likelihoods for features given each class\n",
    "3. Compute posterior probability for each class\n",
    "4. Choose the class with the highest posterior probability\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "'''\n",
    "Feature type of Gaussian NB is Continuous.\n",
    "Feature type of Multinomial NB is Discrete counts.\n",
    "Feature type of Bernouli NB is Binary.\n",
    "Distribution Assumption is Normal for Gaussian.\n",
    "Distribution Assumption is Multinomial for Multinomial NB.\n",
    "Distribution Assumption is Bernoulli for Bernauli NB.\n",
    "Gaussian NB doesnot Handles Frequency.\n",
    "Multinomial NB Handles Frequency.\n",
    "Bernouli NB doesnot Handles Frequency.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6a691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naive Bayes: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "# 10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Gaussian Naive Bayes:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
